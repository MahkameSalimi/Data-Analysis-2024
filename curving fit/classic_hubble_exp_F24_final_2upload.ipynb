{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f3c65f",
   "metadata": {},
   "source": [
    "For more on Hubble's discovery: <br>\n",
    "https://www.nasa.gov/content/about-story-edwin-hubble <br>\n",
    "https://news.uchicago.edu/explainer/hubble-constant-explained\n",
    "\n",
    "Source:\n",
    "http://supernova.lbl.gov/Union/, https://arxiv.org/abs/1105.3470, DOI 10.1088/0004-637X/746/1/85\n",
    "\n",
    "# A Classic Experiment: Edwin Hubble's Discovery of the Expansion of the Universe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ff00c",
   "metadata": {},
   "source": [
    "In the early 20th century, Edwin Hubble (University of Chicago alumnus) significantly contributed to the understanding of the universe via the observation that a number of stars and galaxies moved away from the Earth with a velocity $v$ that is proportional to their distance $d$ from Earth. This relationship is known as Hubble's law\n",
    "\n",
    "$v=H_0 d$\n",
    "\n",
    "in which $H_0$ is the Hubble constant, typically measured in units of km s$^{-1}$ Mpc$^{-1}$ in which Mpc indicates a distance of $10^6$ parsec (1 parsec is about 3.26 light years). The original data used by Hubble can be found in the paper \"The Velocity-Distance Relation among Extra-Galactic Nebulae\" (Hubble, Edwin; Humason, Milton L., 1931). For some time scientists predicted that the value of the Hubble constant should be around 68 km s$^{-1}$ Mpc$^{-1}$. As more data come into light, it has been suggested that the Hubble constant should be around 70 km s$^{-1}$ Mpc$^{-1}$ with some reports pushing it to 72-74 km s$^{-1}$ Mpc$^{-1}$. Distant galaxies moving away faster than nearby galaxies can serve as an evidence for the expansion of the universe and the Hubble constant indicates the rate at which this expansion is happening (see the blueberry muffin baking analogy in the UChicago New cited above).\n",
    "\n",
    "\n",
    "We will attempt to fit two models into the Supernova Union data containing distances versus redshift information. One model is a simple linear law and the second one is a nonlinear (extension) of the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098282d6-8207-4807-8923-c8141ce2725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for data manipulation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Reading csv sample data set into a DataFrame df\n",
    "label='sn_z_mu_dmu_plow_union2.1.txt'\n",
    "df = pd.read_csv(label, delimiter='\\t', names=[\"name\", \"redshift\", \"distance modulus\", \"distance modulus uncertainty\", \"unknown constant\"])\n",
    "\n",
    "# Display how many rows (records) and columns (values per record) we have\n",
    "print ('Data contains %i rows and %i columns' % (df.shape[0], df.shape[1]))\n",
    "df.shape\n",
    "\n",
    "# Display information about our data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338183d-6672-4bea-8f50-4b5f1a4c30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 records from our dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bee6f1-3e47-4b5b-8755-f87abffdba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 5 records from our dataset\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9a332-5af7-4f62-b6fd-b0bed98f8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the data\n",
    "plt.figure()\n",
    "plt.xlabel('redshift(z)', fontsize=15) #Label x\n",
    "plt.ylabel('distance modulus', fontsize=15)#Label y\n",
    "# plot with errorbars\n",
    "plt.errorbar(df['redshift'],df['distance modulus'],yerr=df['distance modulus uncertainty'],marker='.',linestyle = 'None', color = 'black')\n",
    "# uncomment the line below to zoom at small redshifts\n",
    "plt.xlim([0,0.1])\n",
    "plt.ylim(top=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d8fd5-0c0d-4644-9316-c3c6b4a89c9f",
   "metadata": {},
   "source": [
    "The data is stored in terms of 'distance modulus', $\\mu$, (https://en.wikipedia.org/wiki/Distance_modulus) and we need convert it to just 'distance' in parsec. For that, we use the definition\n",
    "\n",
    "$d = 10^{(\\mu/5) + 1}$\n",
    "\n",
    "and the uncertainty in $\\mu$, $\\sigma_\\mu$ also needs to be propagated so we can obtain the uncertainty in $d$, $\\sigma_d$. This can be done using error propagation as\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma_d &=& \\frac{\\partial d}{\\partial \\mu}\\, \\sigma_\\mu \\\\\n",
    " &=& \\frac{\\log(10)}{5}\\, 10^{(\\mu/5)+1}\\, \\sigma_\\mu\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea7d24-2311-41d2-9478-23e5dac3192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's copy the original dataframe to a new one since we will make column alterations\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Let's rename the column to simply 'distance' and make the conversion\n",
    "df_copy = df_copy.rename(columns={'distance modulus': 'distance', 'distance modulus uncertainty': 'd-uncertainty'})\n",
    "\n",
    "# Applying the condition using apply and lambda\n",
    "df_copy['distance'] = 10**(df['distance modulus']/5 + 1) #apply method option: df['distance modulus'].apply(lambda x: 10**(x/5 + 1))\n",
    "df_copy['d-uncertainty'] = (math.log(10)/5)*df_copy['distance']*df['distance modulus uncertainty']\n",
    "\n",
    "# We will keep just the rows where z < 0.1\n",
    "df_reduced = df_copy.loc[ df_copy['redshift'] <= 0.1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a9d8f7-44c7-4ceb-afb8-747d1fcb3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replot the new data\n",
    "plt.figure()\n",
    "plt.xlabel('redshift(z)', fontsize=15) #Label x\n",
    "plt.ylabel('distance (parsec)', fontsize=15)#Label y\n",
    "# plot with errorbars\n",
    "plt.errorbar(df_reduced['redshift'],df_reduced['distance'],yerr=df_reduced['d-uncertainty'],marker='.',linestyle = 'None', color = 'black')\n",
    "# uncomment the line below to zoom at small redshifts\n",
    "plt.xlim([0,0.1])\n",
    "plt.ylim([0,0.6e9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8786c5e-1e55-423f-817a-3c0fd5f05eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8418ae-0876-406e-b74e-638a95720190",
   "metadata": {},
   "source": [
    "<h2>Linear Curve Fitting</h2\n",
    "\n",
    "\n",
    "We will apply linear curve fitting method in `lmfit` (https://lmfit.github.io/lmfit-py/index.html). We will perform a linear curve fitting considering that: \n",
    "<br>\n",
    "<ul>\n",
    "    <li>redshift ($z$) is the predictor/independent variable ($X$);</li>\n",
    "    <li>distance ($d$) is the response/dependent variable (that we may want to predict)($Y$).</li>\n",
    "    <li> The data carries incertainty in $Y$.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The result of the linear regression procedure is a <b>linear model</b> of the form $Y = b\\,X + a$ where</p>\n",
    "\n",
    "<ul>\n",
    "    <li>$a$ refers to the <b>intercept</b> of the regression line, in other words: the value of $Y$ when $X$ is 0;</li>\n",
    "    <li>$b$ refers to the <b>slope</b> of the regression line, in other words: the value with which $Y$ changes when $X$ increases by 1 unit.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2e6c8-e57d-4907-81e8-fd387551288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you need to install lmfit\n",
    "#%pip install lmfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e69fc-96dd-4f4b-9d53-802099e20d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfit.models import LinearModel\n",
    "\n",
    "# Linear regression with lmfit\n",
    "model = LinearModel()\n",
    "params = model.guess(df_reduced['distance'], x=df_reduced['redshift'])\n",
    "\n",
    "result = model.fit(df_reduced['distance'], params, x=df_reduced['redshift'], weights=1.0/df_reduced['d-uncertainty'])\n",
    "print(result.fit_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547dd3b-147a-4c62-afe2-e33a5804b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the result of lmfit\n",
    "result.plot_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d78a34",
   "metadata": {},
   "source": [
    "<b>Constants and conversions required to get $H_0$ in [km s$^{-1}$ Mpc$^{-1}$]<b>\n",
    "    \n",
    "Remember that for the linear model, $Y$ is the distance $d$ and $X$ is the redshift $z$. This means that the linear model is written as\n",
    "    \n",
    "\\begin{equation}\n",
    "    d = \\frac{v}{H_0} + C\n",
    "\\end{equation}\n",
    "    \n",
    "with $C$ being any constant and the velocity $v$ is given in terms of the redshift as $v=cz$ ($c$ is the speed of light $\\approx 3\\times 10^5$ km/s). The distance in our data is in parsec (pc) and we need to convert it to Mega parsec (Mpc), i.e. $d_{pc} = 10^{-6}d_{Mpc}$. Therefore, we can write the relation above as\n",
    "    \n",
    "\\begin{equation}\n",
    "    d_{Mpc} = \\frac{3\\times 10^5 \\times 10^6}{H_0} z + C\n",
    "\\end{equation}\n",
    "    \n",
    "The linear fiting routine therefore has a slope defined as \n",
    "    \n",
    "\\begin{equation}\n",
    "    \\mbox{slope} = \\frac{3\\times 10^5 \\times 10^6}{H_0} \n",
    "\\end{equation}\n",
    "    \n",
    "so, if we wish to extract $H_0$ from the fitted slope, we need to make $H_0 = (3\\times 10^5 \\times 10^6)/\\mbox{slope}$. \n",
    "    \n",
    "The variance in $H_0$ is obtained using error propagation equation as\n",
    "    \n",
    "\\begin{eqnarray}\n",
    "    \\sigma_{H_0}^2 &=& \\left( \\frac{\\partial H_0}{\\partial\\, \\mbox{slope}} \\right )^2\\sigma_{\\mbox{slope}}^2 \\\\\n",
    "        &=& \\left( \\frac{3\\times 10^5 \\times 10^6}{\\mbox{slope}^2} \\right )^2\\sigma_{\\mbox{slope}}^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "The standard deviation in $H_0$ is given by $\\sigma_{H_0}$ (square root of the variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0748a-5d1a-4ffe-aa7e-4051fb78aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hubble constant taken from the linear fit above done with lmfit\n",
    "# Getting the parameters from the result object\n",
    "parameters = result.params\n",
    "\n",
    "# Hubble constant estimate\n",
    "H0_lmfit = 1e6*3e5/parameters['slope'].value\n",
    "\n",
    "# Uncertainty in the Hubble's constant\n",
    "DH0_lmfit = 1e6*3e5*(parameters['slope'].stderr)/(parameters['slope'].value)**2\n",
    "\n",
    "# print the values\n",
    "print(\"Hubble constant estimated by lmfit (linear model):\", H0_lmfit, '+/-', DH0_lmfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ba73d-ed72-4df2-8e8f-a54345c9c8bc",
   "metadata": {},
   "source": [
    "<b> Extended Hubble law - Nonlinear model </b>\n",
    "\n",
    "In the cell below, we will implement another curve fitting method using the `curve_fit` method in `scipy.optimize`. For this method to work, we need to create and pass the function we wish to fit. We will use `curve_fit` on a linear and a nonlinear (extended) Hubble's law version which contains a quadratic contribution written as\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\frac{x}{H_0}\\left \\{1 + \\frac{(1-q)}{2} x \\right \\}\n",
    "\\end{equation}\n",
    "\n",
    "in which $H_0$ is the Hubble constant and $q$ is a higher-order fitting parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828bdd21-f194-4126-9e47-58d032962c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats\n",
    "\n",
    "# Fitting with curve_fit of scipy.optimize\n",
    "def linear_model(x,a,b):\n",
    "    \"\"\"\n",
    "    Linear model for curve fitting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        The x-values.\n",
    "    a : float\n",
    "        The intercept of the line.\n",
    "    b : float\n",
    "        The slope of the line.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : array-like\n",
    "        The y-values.\n",
    "    \"\"\"    \n",
    "    return a + b*x\n",
    "\n",
    "def nonlinear(x,h0,q):\n",
    "    \"\"\"\n",
    "    Modified slope expansion model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        The x-values.\n",
    "    h0 : float\n",
    "         Hubble constant.\n",
    "    q : float\n",
    "        q-parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : array-like\n",
    "        The y-values.\n",
    "    \"\"\"    \n",
    "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n",
    "    return val\n",
    "\n",
    "# passing Pandas dataframe to numpy\n",
    "Y = df_reduced['distance'].to_numpy()\n",
    "X = df_reduced['redshift'].to_numpy()\n",
    "dY = df_reduced['d-uncertainty'].to_numpy()\n",
    "\n",
    "# Fitting with extra argument flags (absolute_sigma)\n",
    "# We will use absolute_sigma = True since we are assuming that the errors are well-defined.\n",
    "\n",
    "# Fitting of the linear model\n",
    "poptlin, pcovlin, infodict, mesg, ier = curve_fit(linear_model, X, Y, sigma=dY, absolute_sigma=True, full_output=True)\n",
    "a, b = poptlin\n",
    "h0 = 1e6*3e5/b #Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n",
    "sigma_a, sigma_b = np.sqrt(np.diag(pcovlin))\n",
    "sigma_h0 = 1e6*3e5*sigma_b/b**2\n",
    "\n",
    "# Print the results\n",
    "print(\"Linear Model parameters\")\n",
    "print(\"----------------\")\n",
    "print(f\"a = {h0:.3f} +/- {sigma_h0:.3f}\")\n",
    "print(f\"b = {b:.3f} +/- {sigma_b:.3f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Fitting of the nonlinear model\n",
    "poptnl, pcovnl, infodict, mesg, ier = curve_fit(nonlinear, X, Y, sigma=dY, absolute_sigma=True, full_output=True)\n",
    "# Extract the best-fit parameters and their uncertainties with extra argument flags\n",
    "h0, q = poptnl\n",
    "sigma_h0, sigma_q = np.sqrt(np.diag(pcovnl))\n",
    "\n",
    "# Print the results\n",
    "print(\"Nonlinear Model parameters\")\n",
    "print(\"----------------\")\n",
    "print(f\"h0 = {h0:.3f} +/- {sigma_h0:.3f}\")\n",
    "print(f\"q = {q:.3f} +/- {sigma_q:.3f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757e641",
   "metadata": {},
   "source": [
    "# Note on `absolute_sigma` flag in `curve_fit`\n",
    "\n",
    "In the `scipy.optimize.curve_fit` function, the parameter `absolute_sigma` controls how the uncertainty (standard deviation) in the data is treated during the fitting process.\n",
    "\n",
    "If `absolute_sigma=False` (default), the uncertainties provided via the sigma parameter are interpreted as relative weights. This means the uncertainties are normalized by the residual sum of squares of the fit, which results in scaling the covariance matrix by a factor dependent on the goodness of fit.\n",
    "\n",
    "If `absolute_sigma=True`, the uncertainties provided in sigma are treated as absolute standard deviations, meaning that the values in sigma are taken at face value and directly influence the weighting in the least-squares optimization. The covariance matrix returned from `curve_fit` will not be scaled by the residuals in this case.\n",
    "\n",
    "Key Points:\n",
    "Use `absolute_sigma=True` if the uncertainties in your data points (given by sigma) are well-established, and you want the fit to respect those exact uncertainties.\n",
    "Use `absolute_sigma=False` (default) if the uncertainties are relative or approximate, and you want the fitting process to account for the fit quality when estimating the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf9bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replot the reduced data with the fitted model of curve_fit\n",
    "plt.figure()\n",
    "plt.xlabel('redshift(z)', fontsize=15) #Label x\n",
    "plt.ylabel('distance (parsec)', fontsize=15)#Label y\n",
    "# plot of the models\n",
    "dx = 0.01\n",
    "xrange = np.arange(min(X),0.1,dx)\n",
    "plt.plot(xrange, a + b*xrange, label='Linear Model',color='b',zorder=1)\n",
    "plt.plot(xrange, nonlinear(xrange,h0,q), label='Nonlinear Model',color='r',zorder=1)\n",
    "# plot data with errorbars\n",
    "plt.errorbar(df_reduced['redshift'],df_reduced['distance'],yerr=df_reduced['d-uncertainty'],marker='.',label='data',linestyle = 'None', color = 'black',zorder=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9876b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the nonlinear fit at this small redshift range, we will obtain the individual residuals and plot them\n",
    "def plotHist(residuals,x):\n",
    "    nbins = 30.0\n",
    "    y0, bin_edges = np.histogram(residuals, bins=int(nbins))\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    norm0=len(residuals)*(bin_edges[-1]-bin_edges[0])/nbins\n",
    "    \n",
    "    fig, axs = plt.subplots(2,1, figsize=(6, 6), layout='constrained')\n",
    "    axs[0].plot(residuals, x, 'k.')\n",
    "    axs[0].set_ylabel('redshift(z)', fontsize=15)\n",
    "    axs[0].set_xlabel('residuals', fontsize=15)\n",
    "    \n",
    "    axs[1].errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid')\n",
    "    axs[1].set_xlabel('residuals', fontsize=15)\n",
    "    axs[1].set_ylabel('Probability', fontsize=15)\n",
    "    \n",
    "    \n",
    "    #for good measure, let's compare this to a gaussian distribution\n",
    "    k=np.linspace(bin_edges[0],bin_edges[-1],100)\n",
    "    normal=stats.norm.pdf(k,0,residuals.std())\n",
    "    axs[1].plot(k,normal,'-',label='Gaussian')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Calculating residuals\n",
    "residuals = (df_reduced['distance'] - nonlinear(df_reduced['redshift'],h0,q))/df_reduced['d-uncertainty']\n",
    "plotHist(residuals,df_reduced['redshift'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225753b0",
   "metadata": {},
   "source": [
    "<b>Histogram error bars<b>\n",
    "    \n",
    "The error bars in binned (uncorrelated) data can be estimated by considering that the number of entries $h_i$ in each bin $i$ is a random Poisson-distributed with an average value $\\mu_i = N p_i$ with $p_i$ being\n",
    "    \n",
    "\\begin{equation}\n",
    "    \\lim_{N\\rightarrow \\infty}\\, \\frac{h_i}{N} = p_i\n",
    "\\end{equation}\n",
    "    \n",
    "$N$ being the number of measurements, and the variance $\\mbox{Var}[h_i] = \\mu_i \\approx h_i $. So, the standard deviation is $\\sigma_i = \\sqrt(h_i)$. This can be normalized by the total number of observations giving $\\sqrt(h_i)/N$.\n",
    "    \n",
    "<b>Resisual analysis<b>\n",
    "\n",
    "The plot shows that they are Gaussian distributed, indicating a relatively good fit. Note that the Gaussian plotted in the figure is not a fit, just manually built for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit the nonlinear model over the whole data\n",
    "# passing Pandas dataframe to numpy\n",
    "Y = df_copy['distance'].to_numpy()\n",
    "X = df_copy['redshift'].to_numpy()\n",
    "dY = df_copy['d-uncertainty'].to_numpy()\n",
    "\n",
    "# Fitting with extra argument flags (absolute_sigma)\n",
    "# We will use absolute_sigma = True since we are assuming that the errors are well-defined.\n",
    "#popt, pcov, infodict, mesg, ier = curve_fit(linear_model, X, Y, sigma=dY, absolute_sigma=True, full_output=True)\n",
    "popt, pcov, infodict, mesg, ier = curve_fit(nonlinear, X, Y, sigma=dY, absolute_sigma=True, full_output=True)\n",
    "# Extract the best-fit parameters and their uncertainties with extra argument flags\n",
    "h0, q = popt\n",
    "sigma_h0, sigma_q = np.sqrt(np.diag(pcov))\n",
    "\n",
    "# Print the results\n",
    "print(\"Nonlinear Model parameters\")\n",
    "print(\"----------------\")\n",
    "print(f\"h0 = {h0:.3f} +/- {sigma_h0:.3f}\")\n",
    "print(f\"q = {q:.3f} +/- {sigma_q:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the fit over the whole data\n",
    "plt.figure()\n",
    "plt.xlabel('redshift(z)', fontsize=15) #Label x\n",
    "plt.ylabel('distance (parsec)', fontsize=15)#Label y\n",
    "dx = 0.01\n",
    "xrange = np.arange(min(X),max(X),dx)\n",
    "# plot of the models\n",
    "plt.plot(xrange, nonlinear(xrange,h0,q), label='Nonlinear Model',color='b',zorder=1)\n",
    "plt.plot(xrange, linear_model(xrange,a,b), label='Linear Model',color='r',zorder=2)\n",
    "# plot data with errorbars\n",
    "plt.errorbar(df_copy['redshift'],df_copy['distance'],yerr=df_copy['d-uncertainty'],marker='.',label='data',linestyle = 'None', color = 'black',zorder=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64c977-b67e-4ffa-a8fa-d302a087881c",
   "metadata": {},
   "source": [
    "<b> IMPORTANT:</b> Note that the linear fit above is the linear model that was only fitted to the low range of redshift values! Ideally, we would need to repeat the fitting process with the linear model over the whole data! Nonetheless, we did not do that just to illustrate a case of a \"bad fit\" and visualize the shift/bias in the residuals later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0d892",
   "metadata": {},
   "source": [
    "<b>$\\chi^2$ comparison for goodness of fit<b>\n",
    "\n",
    "We can have an idea of how good the fit is by comparing the minimum value of $\\chi^2$ with the number of degrees of freedom (DoF) given by $n - p$ with $n$ being the number of points in the data and $p$ the number of floating parameters in the model. A reasonably good fit will result in $\\chi^2/DoF \\approx 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb478e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the minimum chi^2 value obtained by curve_fit divided by DoF\n",
    "print(f\"SciPy curve_fit \\chi^2/DoF: {np.sum(infodict['fvec']**2)/(len(df_copy.index)-2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating residuals obtained for the whole dataset (but with the linear model!)\n",
    "residuals_complete = (df_copy['distance'] - linear_model(df_copy['redshift'],a,b))/df_copy['d-uncertainty']\n",
    "plotHist(residuals_complete,df_copy['redshift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42169b",
   "metadata": {},
   "source": [
    "<b>Residual Analysis<b>\n",
    "\n",
    "The residual plot shows many points tending to a direction. That is indicative that the linear fit does not explain the whole data adequately. Note that the Gaussian drawn in the plot is not a fitting, that is just manually plotted as a guideline to show that the residual distribution is not around zero-mean anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8505a1-819a-4937-89df-6611499d4406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
